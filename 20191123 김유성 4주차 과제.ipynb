{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDa48t8OFSp5sUVzhM29zf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"IhB5n_FoX8-l","executionInfo":{"status":"ok","timestamp":1679725073721,"user_tz":-540,"elapsed":1,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}}},"outputs":[],"source":["from nltk import FreqDist\n","import re\n","import numpy as np\n","\n","def buildDict(docs):\n","    doc_tokens = []     # python list\n","    for doc in docs:\n","        delim = re.compile(r'[\\s,.]+')\n","        tokens = delim.split(doc.lower()) \n","        if tokens[-1] == '' :   tokens = tokens[:-1] \n","        doc_tokens.append(tokens)\n","\n","    vocab = FreqDist(np.hstack(doc_tokens))\n","    vocab = vocab.most_common()\n","    word_to_id = {word[0] : id for id, word in enumerate(vocab)}\n","    id_to_word = {id : word[0] for id, word in enumerate(vocab)}\n","    return doc_tokens, vocab, word_to_id, id_to_word\n","\n","docs = []\n","docs.append('To do is to be. To be is to do.')\n","docs.append('To be or not to be. I am what I am')\n","docs.append('I think therefore I am. Do be do be do.')\n","docs.append('Do do do da da da. Let it be let it be.')\n","\n","doc_tokens, vocab, word_to_id, id_to_word = buildDict(docs)\n","\n","doc_vectors = [] #4개 문서의 벡터\n","for tokens in doc_tokens:                           \n","    doc_vector = []                             #문서별 벡터생성(행렬)\n","    for token in tokens:                            \n","        one_hot_vector = [0 for _ in word_to_id] #사전 크기 벡터 생성\n","        one_hot_vector[word_to_id[token]] = 1    #사전에서 토큰 검색 후 해당 인덱스만 1로\n","        doc_vector.append(one_hot_vector)        #문서별 벡터에 단어벡터 추가\n","    doc_vectors.append(doc_vector)          #4개 문서의 벡터에 한 문서벡터 추가"]},{"cell_type":"code","source":["#===========실습1 원문 복구===========#\n","import pandas as pd\n","\n","for doc_vector in doc_vectors:\n","  words = []\n","  for vector in doc_vector:\n","    index = vector.index(1)\n","    words.append(id_to_word[index])\n","    sentence = \" \".join(words)\n","  print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9E1sGiJ8YTca","executionInfo":{"status":"ok","timestamp":1679725078815,"user_tz":-540,"elapsed":486,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"a73ce0ab-4b6e-4315-c0c9-4dc4da45a1da"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["to do is to be to be is to do\n","to be or not to be i am what i am\n","i think therefore i am do be do be do\n","do do do da da da let it be let it be\n"]}]},{"cell_type":"code","source":["#===========실습2 이진 벡터===========#\n","doc_vectors = []                               #이진벡터\n","for tokens in doc_tokens:\n","    doc_vector = [0 for _ in word_to_id]       #문서별 벡터\n","    for token in tokens: \n","        if token in word_to_id:\n","            doc_vector[word_to_id[token]] = 1   #사전에 있는 단어는 1로 설정\n","\n","    doc_vectors.append(doc_vector)             #이진벡터에 문서벡터 추가\n","\n","import pandas as pd\n","\n","df = pd.DataFrame(doc_vectors, columns=id_to_word.values())\n","print(df.T)\n","\n","word = input(\"input keyword : \")\n","cnt = 0\n","for doc_vector in doc_vectors:\n","  if doc_vector[word_to_id[word]] == 1:\n","    print(\"doc_num = {}. Text : {}\".format(cnt,docs[cnt]))\n","  cnt += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeuM224_aZwM","executionInfo":{"status":"ok","timestamp":1679725564063,"user_tz":-540,"elapsed":5068,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"a0124165-17a4-411f-85f6-de4a157c362c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["           0  1  2  3\n","do         1  0  1  1\n","be         1  1  1  1\n","to         1  1  0  0\n","i          0  1  1  0\n","am         0  1  1  0\n","da         0  0  0  1\n","is         1  0  0  0\n","let        0  0  0  1\n","it         0  0  0  1\n","or         0  1  0  0\n","not        0  1  0  0\n","what       0  1  0  0\n","think      0  0  1  0\n","therefore  0  0  1  0\n","input keyword : or\n","doc_num = 1. Text : To be or not to be. I am what I am\n"]}]}]}
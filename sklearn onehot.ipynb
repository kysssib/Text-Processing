{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGp2yid6i0U2XQ/1T0PJKp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"4gwcC-cQuSZC","executionInfo":{"status":"ok","timestamp":1680418104806,"user_tz":-540,"elapsed":2460,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}}},"outputs":[],"source":["#사전 구축\n","from nltk import FreqDist\n","import numpy as np\n","import re\n","\n","def buildDict(docs):\n","    doc_tokens = []     # python list\n","    for doc in docs:\n","        delim = re.compile(r'[\\s,.]+')\n","        tokens = delim.split(doc.lower()) \n","        if tokens[-1] == '' :   tokens = tokens[:-1] \n","        doc_tokens.append(tokens)\n","\n","        \n","    vocab = FreqDist(np.hstack(doc_tokens))\n","    vocab = vocab.most_common()\n","    word_to_id = {word[0] : id for id, word in enumerate(vocab)}\n","    id_to_word = {id : word[0] for id, word in enumerate(vocab)}\n","    return doc_tokens, vocab, word_to_id, id_to_word\n","\n","docs = []\n","docs.append('To do is to be. To be is to do.')\n","docs.append('To be or not to be. I am what I am')\n","docs.append('I think therefore I am. Do be do be do.')\n","docs.append('Do do do da da da. Let it be let it be.')\n","\n","doc_tokens, vocab, word_to_id, id_to_word = buildDict(docs)"]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","\n","encoder = LabelEncoder()\n","labels = encoder.fit_transform([word for word, id in vocab])\n","for label in labels:\n","    print('[{:2d} : {}]'.format(label, encoder.classes_[label]))\n","print(encoder.classes_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZDjeG9YuaBk","executionInfo":{"status":"ok","timestamp":1680418191452,"user_tz":-540,"elapsed":327,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"0d753ece-f5af-49fe-bf94-a46d8fc8a73a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 3 : do]\n","[ 1 : be]\n","[12 : to]\n","[ 4 : i]\n","[ 0 : am]\n","[ 2 : da]\n","[ 5 : is]\n","[ 7 : let]\n","[ 6 : it]\n","[ 9 : or]\n","[ 8 : not]\n","[13 : what]\n","[11 : think]\n","[10 : therefore]\n","['am' 'be' 'da' 'do' 'i' 'is' 'it' 'let' 'not' 'or' 'therefore' 'think'\n"," 'to' 'what']\n"]}]},{"cell_type":"code","source":["encode_data = np.array([encoder.transform(doc_token) for doc_token in doc_tokens])\n","        \n","print(encode_data)\n","for code in encode_data:\n","    print(encoder.inverse_transform(code))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_bMKwT9vKjm","executionInfo":{"status":"ok","timestamp":1680418388784,"user_tz":-540,"elapsed":318,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"897d4a5d-57e2-4ad5-e11a-d07570630921"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([12,  3,  5, 12,  1, 12,  1,  5, 12,  3])\n"," array([12,  1,  9,  8, 12,  1,  4,  0, 13,  4,  0])\n"," array([ 4, 11, 10,  4,  0,  3,  1,  3,  1,  3])\n"," array([3, 3, 3, 2, 2, 2, 7, 6, 1, 7, 6, 1])]\n","['to' 'do' 'is' 'to' 'be' 'to' 'be' 'is' 'to' 'do']\n","['to' 'be' 'or' 'not' 'to' 'be' 'i' 'am' 'what' 'i' 'am']\n","['i' 'think' 'therefore' 'i' 'am' 'do' 'be' 'do' 'be' 'do']\n","['do' 'do' 'do' 'da' 'da' 'da' 'let' 'it' 'be' 'let' 'it' 'be']\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-1e5a7ff9b463>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  encode_data = np.array([encoder.transform(doc_token) for doc_token in doc_tokens])\n"]}]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","oh_encoder = OneHotEncoder(categories='auto')\n","labels = labels.reshape(-1, 1)\n","print(labels)\n","\n","oh_labels = oh_encoder.fit_transform(labels)\n","\n","print(oh_labels.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3Mtl0Ruvzlq","executionInfo":{"status":"ok","timestamp":1680418655992,"user_tz":-540,"elapsed":811,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"6ec57f19-e0ad-49f4-c406-1197105558e7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 3]\n"," [ 1]\n"," [12]\n"," [ 4]\n"," [ 0]\n"," [ 2]\n"," [ 5]\n"," [ 7]\n"," [ 6]\n"," [ 9]\n"," [ 8]\n"," [13]\n"," [11]\n"," [10]]\n","[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"]}]},{"cell_type":"code","source":["oh_vectors = []\n","for data in encode_data:\n","    data = data.reshape(-1,1)\n","    oh_vector = oh_encoder.transform(data).toarray()\n","    oh_vectors.append(oh_vector)\n","\n","for data, oh_vector in zip(encode_data, oh_vectors):\n","    print(encoder.inverse_transform(data))\n","    print(oh_vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"seZh73XIwkf-","executionInfo":{"status":"ok","timestamp":1680418882271,"user_tz":-540,"elapsed":345,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"77c4e64d-61dd-4d1a-c5d1-e157abbb7f15"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['to' 'do' 'is' 'to' 'be' 'to' 'be' 'is' 'to' 'do']\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","['to' 'be' 'or' 'not' 'to' 'be' 'i' 'am' 'what' 'i' 'am']\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","['i' 'think' 'therefore' 'i' 'am' 'do' 'be' 'do' 'be' 'do']\n","[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","['do' 'do' 'do' 'da' 'da' 'da' 'let' 'it' 'be' 'let' 'it' 'be']\n","[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","docs = []\n","docs.append('To do is to be. To be is to do.')\n","docs.append('To be or not to be. II am what II am')\n","docs.append('II think therefore II am. Do be do be do.')\n","docs.append('Do do do da da da. Let it be let it be.')\n","\n","cnt_vectr = CountVectorizer()\n","vectors = cnt_vectr.fit_transform(docs)\n","\n","print(cnt_vectr.vocabulary_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"396AIXsvxUw8","executionInfo":{"status":"ok","timestamp":1680418978483,"user_tz":-540,"elapsed":2,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"dd916114-256e-4bdc-b4e0-5cd5c170eb56"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["{'to': 12, 'do': 3, 'is': 5, 'be': 1, 'or': 9, 'not': 8, 'ii': 4, 'am': 0, 'what': 13, 'think': 11, 'therefore': 10, 'da': 2, 'let': 7, 'it': 6}\n"]}]},{"cell_type":"code","source":["print(cnt_vectr.get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aD97jB-bxn4w","executionInfo":{"status":"ok","timestamp":1680418980448,"user_tz":-540,"elapsed":420,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"98c511fd-866c-4476-ab29-54a32a65a5ca"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['am' 'be' 'da' 'do' 'ii' 'is' 'it' 'let' 'not' 'or' 'therefore' 'think'\n"," 'to' 'what']\n"]}]},{"cell_type":"code","source":["print(vectors.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLvqAwV2xszv","executionInfo":{"status":"ok","timestamp":1680418990733,"user_tz":-540,"elapsed":423,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"3a979935-e94b-42f8-facd-ae5774353f02"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 2 0 2 0 2 0 0 0 0 0 0 4 0]\n"," [2 2 0 0 2 0 0 0 1 1 0 0 2 1]\n"," [1 2 0 3 2 0 0 0 0 0 1 1 0 0]\n"," [0 2 3 3 0 0 2 2 0 0 0 0 0 0]]\n"]}]},{"cell_type":"code","source":["print(pd.DataFrame(vectors.toarray(),\n","                   columns=cnt_vectr.get_feature_names_out()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZG_sd_Swxum_","executionInfo":{"status":"ok","timestamp":1680419004707,"user_tz":-540,"elapsed":638,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"123ec186-f222-4c3d-9f28-055bcde44595"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["   am  be  da  do  ii  is  it  let  not  or  therefore  think  to  what\n","0   0   2   0   2   0   2   0    0    0   0          0      0   4     0\n","1   2   2   0   0   2   0   0    0    1   1          0      0   2     1\n","2   1   2   0   3   2   0   0    0    0   0          1      1   0     0\n","3   0   2   3   3   0   0   2    2    0   0          0      0   0     0\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","tfidf = TfidfVectorizer().fit(docs)\n","dtm = tfidf.transform(docs).toarray()\n","\n","df = pd.DataFrame(dtm, columns=tfidf.get_feature_names_out())\n","print(df)\n","print(sorted(tfidf.vocabulary_.items()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOLQ0DFUx_ao","executionInfo":{"status":"ok","timestamp":1680419075130,"user_tz":-540,"elapsed":331,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"f3547d89-f4ab-4def-e7aa-19f472a41098"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["         am        be        da        do        ii        is        it  \\\n","0  0.000000  0.255666  0.000000  0.312717  0.000000  0.489931  0.000000   \n","1  0.464005  0.307120  0.000000  0.000000  0.464005  0.000000  0.000000   \n","2  0.251031  0.332310  0.000000  0.609695  0.502063  0.000000  0.000000   \n","3  0.000000  0.223758  0.643179  0.410533  0.000000  0.000000  0.428786   \n","\n","        let       not        or  therefore     think        to      what  \n","0  0.000000  0.000000  0.000000   0.000000  0.000000  0.772535  0.000000  \n","1  0.000000  0.294266  0.294266   0.000000  0.000000  0.464005  0.294266  \n","2  0.000000  0.000000  0.000000   0.318401  0.318401  0.000000  0.000000  \n","3  0.428786  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n","[('am', 0), ('be', 1), ('da', 2), ('do', 3), ('ii', 4), ('is', 5), ('it', 6), ('let', 7), ('not', 8), ('or', 9), ('therefore', 10), ('think', 11), ('to', 12), ('what', 13)]\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","tfidf = TfidfVectorizer(norm='l1').fit(docs)\n","dtm = tfidf.transform(docs).toarray()\n","print(dtm)\n","\n","df = pd.DataFrame(dtm, columns=tfidf.get_feature_names_out())\n","print(df)\n","print(sorted(tfidf.vocabulary_.items()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HytW7tVPyx_f","executionInfo":{"status":"ok","timestamp":1680419350522,"user_tz":-540,"elapsed":314,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"341b5a71-be59-46f7-a798-71d2d22fb149"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.13964363 0.         0.17080421 0.         0.2675978\n","  0.         0.         0.         0.         0.         0.\n","  0.42195436 0.        ]\n"," [0.1797123  0.11894973 0.         0.         0.1797123  0.\n","  0.         0.         0.11397113 0.11397113 0.         0.\n","  0.1797123  0.11397113]\n"," [0.10765092 0.14250608 0.         0.26145809 0.21530184 0.\n","  0.         0.         0.         0.         0.13654154 0.13654154\n","  0.         0.        ]\n"," [0.         0.10480277 0.30124885 0.19228324 0.         0.\n","  0.20083257 0.20083257 0.         0.         0.         0.\n","  0.         0.        ]]\n","         am        be        da        do        ii        is        it  \\\n","0  0.000000  0.139644  0.000000  0.170804  0.000000  0.267598  0.000000   \n","1  0.179712  0.118950  0.000000  0.000000  0.179712  0.000000  0.000000   \n","2  0.107651  0.142506  0.000000  0.261458  0.215302  0.000000  0.000000   \n","3  0.000000  0.104803  0.301249  0.192283  0.000000  0.000000  0.200833   \n","\n","        let       not        or  therefore     think        to      what  \n","0  0.000000  0.000000  0.000000   0.000000  0.000000  0.421954  0.000000  \n","1  0.000000  0.113971  0.113971   0.000000  0.000000  0.179712  0.113971  \n","2  0.000000  0.000000  0.000000   0.136542  0.136542  0.000000  0.000000  \n","3  0.200833  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n","[('am', 0), ('be', 1), ('da', 2), ('do', 3), ('ii', 4), ('is', 5), ('it', 6), ('let', 7), ('not', 8), ('or', 9), ('therefore', 10), ('think', 11), ('to', 12), ('what', 13)]\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","tfidf = TfidfVectorizer(norm='l2').fit(docs)\n","dtm = tfidf.transform(docs).toarray()\n","print(dtm)\n","\n","df = pd.DataFrame(dtm, columns=tfidf.get_feature_names_out())\n","print(df)\n","print(sorted(tfidf.vocabulary_.items()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVG_bApey89I","executionInfo":{"status":"ok","timestamp":1680419345569,"user_tz":-540,"elapsed":761,"user":{"displayName":"김유성 (Kuru)","userId":"06118974840308071375"}},"outputId":"7f51d683-c78e-4cea-ec56-c40a570b2ad9"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.25566647 0.         0.3127168  0.         0.48993129\n","  0.         0.         0.         0.         0.         0.\n","  0.77253491 0.        ]\n"," [0.4640046  0.30711989 0.         0.         0.4640046  0.\n","  0.         0.         0.2942655  0.2942655  0.         0.\n","  0.4640046  0.2942655 ]\n"," [0.25103134 0.33231014 0.         0.6096945  0.50206267 0.\n","  0.         0.         0.         0.         0.31840142 0.31840142\n","  0.         0.        ]\n"," [0.         0.22375843 0.6431793  0.41053301 0.         0.\n","  0.4287862  0.4287862  0.         0.         0.         0.\n","  0.         0.        ]]\n","         am        be        da        do        ii        is        it  \\\n","0  0.000000  0.255666  0.000000  0.312717  0.000000  0.489931  0.000000   \n","1  0.464005  0.307120  0.000000  0.000000  0.464005  0.000000  0.000000   \n","2  0.251031  0.332310  0.000000  0.609695  0.502063  0.000000  0.000000   \n","3  0.000000  0.223758  0.643179  0.410533  0.000000  0.000000  0.428786   \n","\n","        let       not        or  therefore     think        to      what  \n","0  0.000000  0.000000  0.000000   0.000000  0.000000  0.772535  0.000000  \n","1  0.000000  0.294266  0.294266   0.000000  0.000000  0.464005  0.294266  \n","2  0.000000  0.000000  0.000000   0.318401  0.318401  0.000000  0.000000  \n","3  0.428786  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n","[('am', 0), ('be', 1), ('da', 2), ('do', 3), ('ii', 4), ('is', 5), ('it', 6), ('let', 7), ('not', 8), ('or', 9), ('therefore', 10), ('think', 11), ('to', 12), ('what', 13)]\n"]}]}]}